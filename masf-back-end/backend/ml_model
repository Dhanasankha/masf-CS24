# -*- coding: utf-8 -*-
"""ml_model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13VV07fIwt82iYQTOBrOUmk4i4T2KQlX_
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
#Make NumPy printouts easier to read.

import tensorflow as tf
from tensorflow import keras
cvbfrom keras import preprocessing

print(tf.__version__)

# Defining the column mappings for each CSV file
col_mapping_1 = {'track_name': 'app_name', 'prime_genre': 'category', 'price': 'price', 'size_bytes':'size','user_rating':'rating','rating_count_tot':'rating_count'}
col_mapping_2 = {'App': 'app_name', 'Category': 'category', 'Price': 'price','Size':'size', 'Rating':'rating','Reviews':'rating_count'}

# Defining the data types for the columns
dtypes = {'app_name': str, 'category': str,'price':float,'size':int,'rating':int,'rating':float,'rating_count':int}

# Load the CSV files into pandas dataframes with the column mappings and data types
df1 = pd.read_csv('AppleStore.csv', usecols=col_mapping_1.keys(), dtype=dtypes)
df1 = df1.rename(columns=col_mapping_1)
df2 = pd.read_csv('googleplaystore.csv', usecols=col_mapping_2.keys(), dtype=dtypes)
df2 = df2.rename(columns=col_mapping_2)

# Merge the two dataframes
merged_df = pd.concat([df1, df2], ignore_index=True)
print(merged_df)

dataset = merged_df.copy()
dataset.tail()

dataset.isna().sum() #to find missing values

dataset = dataset.dropna() 
#to remove missing values

print(dataset.columns)
# One-hot encode categorical features
categorical_cols = ['app_name', 'size', 'price', 'rating_count', 'category']
for col in categorical_cols:
    one_hot = pd.get_dummies(dataset[col], prefix=col)
    dataset = dataset.drop(col, axis=1)
    dataset = pd.concat([dataset, one_hot], axis=1)
    dataset.tail()

print(dataset.columns)

train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)
encoded_cols = list(train_dataset.columns)[4:-1]
new_cols = ['size', 'price', 'rating_count', 'rating'] + [col.split('_', 1)[0] for col in encoded_cols]
new_col_names = new_cols + ['category']
train_dataset.columns = new_col_names
train_dataset = train_dataset.loc[:,~train_dataset.columns.duplicated()] #remove duplicate columns

print(train_dataset.shape)
print(train_dataset.columns)

sns.pairplot(train_dataset[['size', 'price','rating','category']], diag_kind='kde')

train_dataset.describe().transpose()

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('rating')
test_labels = test_features.pop('rating')

train_dataset.describe().transpose()[['mean', 'std']]

normalizer = tf.keras.layers.Normalization(axis=-1)

normalizer.adapt(np.array(train_features))

print(normalizer.mean.numpy())

first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())
